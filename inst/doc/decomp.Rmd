---
title: "Kinship Decomposition"
author: "Brian S. Yandell"
date: "2/21/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The purpose of this is to develop math to modify R/qtl2scan so that the kinship object can be decomposed once and used multiple times. The challenge is that the current setup decomposes kinship on each reduced set of data, chosen to eliminate for instance individuals with missing data for a phenotype, or a subset of individuals to improve computation.

The basic math for complete data is as follows.

$$y = X\beta + g + e$$

with
$\mu_q=X\beta$ = QTL effects (and any other fixed effects), 
$g$ = polygenic effects (random), 
e = unexplained variation (random) and distributions

$$g\sim N(0,\sigma_g^2 K), \quad e\sim N(0,\sigma^2 I)$$

Here,
$K$ = kinship matrix and
$I$ = identity matrix (1s on diagonal, 0s off diagonal).
Put another way, the distribution of phenotypes is

$$y \sim N(X\beta,V), \quad V =\sigma_g^2K + \sigma^2I$$

The MLEs are found by iterating to solve (similar to EM idea),
getting MLE of $\beta$ given $V$,
$\hat{\beta}_q=(X^{\text{T}}V^{-1}X)^{-1}X^{\text{T}}V^{-1/2}y$, and
estimating $\sigma_g$ and $\sigma^2$ given $\hat{\beta}$

## SVD of $K$ in full data case

Rather than working with $V$ directly, the $K$ matrix is decomposed using SVD as

$$K = UDU^{\text{T}}$$

with $U$ orthonormal (eigenvector columns are uncorrelated with variance 1; $U^{\text{T}}U=UU^{\text{T}}=I$). The $D$ matrix has 0 off-diagonal and diagonal entries being the eigenvalues $d_i$. With $\gamma = \sigma_g^2 / \sigma^2$, we can write $V$ as

$$V = \sigma^2 (\gamma UDU^{\text{T}} +I)$$

Now we transform the problem by left-multiplying by $CU^{\text{T}}$ and right-multiplying by $UC$, with $C$ being diagonal with entries $c_i=(1+\gamma d_i)^{-1/2}$.

$$CU^{\text{T}}VUC = I$$

That is, the transformed model is

$$y^*=CU^{\text{T}}y \sim N(X^*\beta,I), \quad X^*=CU^{\text{T}}X$$

and the solution is from standard linear models,

$$\hat{\beta}=(X^{*\text{T}}X^*)^{-1}X^{*\text{T}}y^* = (X^{\text{T}}UC^2U^{\text{T}}X)^{-1}X^{\text{T}}UC^2U^{\text{T}}y~.$$

## QR decomposition for subsets

If some phenotypes are missing, or we compute with a subset of individuals, the problem needs to be refactored. Suppose the $n$ vector of phenotypes $y$ is subdivided as $y=(y_1,y_2)$, with $y_2$ being $n_2$ `NA` values. Rather than reduce the size of problem and do another SVD, consider a diagonal matrix $B$ with $n_1$ 1s and $n_2=n-n_1$ 0s down the diagonal. In other words, with $I_1$ an identity matrix of size $n_1$,

$$B = \begin{bmatrix} I_1 & 0 \\ 0 & 0 \end{bmatrix}$$

and $By = (y_1,0)$. 
Note that $B^{\text{T}} = B^2 =B$, but $B$ does not have an inverse. 
Formally, we write

$$B y \sim N(B X\beta,BVB), \quad BVB =\sigma^2(\gamma BKB + B)$$

If we partition $K$ (and similarly $V$) into four parts, we have

$$K = \begin{bmatrix} K_{11} & K_{21} \\ K_{12} & K_{22} \end{bmatrix}~,
\quad BKB = \begin{bmatrix} K_{11} & 0 \\ 0 & 0 \end{bmatrix}~.$$

Rather than perform a _new_ SVD on $K_{11}$, it is more efficient to do a QR decomposition.
The number of calculations for QR are $O(n_1n^2-n^3/3)$ operations vs $O(n_1n^2+n^3)$ for SVD.
That is, QR should be much quicker.
See [Princeton CS Notes](http://www.cs.princeton.edu/courses/archive/fall11/cos323/notes/cos323_f11_lecture09_svd.pdf).

Partition the eigenvector matrix $U$ vertically as 
$U = \begin{bmatrix} U_1\\U_2\end{bmatrix}~.$ 
Here, $U_1U_1^{\text{T}}=I_1$, $U_2U_2^{\text{T}}=I_2$, $U_1U_2^{\text{T}}=0$ and $U_2U_1^{\text{T}}=0$, with $I_j$ being an identity matrix of size $n_j$. Also, $I=U_1^{\text{T}}U_1+U_2^{\text{T}}U_2$. 
Note that $BU = \begin{bmatrix} U_1\\0\end{bmatrix}$ and $UDU^{\text{T}}$ partitions as

$$UDU^{\text{T}} = \begin{bmatrix} U_1DU_1^{\text{T}} & U_2DU_1^{\text{T}} \\ U_1DU_2^{\text{T}} & U_2DU_2^{\text{T}} \end{bmatrix}~,
\quad BKB = BUDU^{\text{T}}B = \begin{bmatrix} U_1DU_1^{\text{T}} & 0 \\ 0 & 0 \end{bmatrix}~.$$

The part we are interested in is

$$y_1 \sim N(X_1\beta,V_{11}), \quad V_{11} =\sigma^2(\gamma K_{11} + I_1)=\sigma^2(\gamma U_1DU_1^{\text{T}} + I_1)=\sigma^2U_1C^{-2}U_1^{\text{T}}$$

Consider a Householder decomposition of $C^{-1}U_1^{\text{T}}$:
$C^{-1}U_1^{\text{T}} = FG=F_1G_1$ with $G_1$ upper triangular and $FF^{\text{T}}=F^{\text{T}}F=I$,
$F=(F_1,F_2)$, $G^{\text{T}}=(G_1^{\text{T}}, 0)$. Also, $F_1F_1^{\text{T}}+F_2F_2^{\text{T}}=I$ and $F_1^{\text{T}}F_1=I_1$.

Hence, $V_{11} = \sigma^2G_1^{\text{T}}F_1^{\text{T}}F_1G_1 = \sigma^2G_1^{\text{T}}G_1$. Thus, the model we need to solve is $y_1^* \sim N(X_1^*\beta,\sigma^2I_1)$, with

$$y_1^* = G_1^{-\text{T}}y_1, \quad X_1^* = G_1^{-\text{T}}X_1$$
and the solution for $\beta$ is:

$$\hat{\beta} = (X_1^{*\text{T}}X_1^*)^{-1}X_1^{*\text{T}}y_1^* = (X_1^{\text{T}}G_1^{-1}G_1^{-\text{T}}X_1)^{-1}X_1^{\text{T}}G_1^{-1}G_1^{-\text{T}}y_1$$

That is, we need to take the current value of $\gamma$, do a Householder decomposition on the reduced matrix $C^{-1}U_1^{\text{T}}$, and solve OLS.

### QR solution for OLS

Note that the OLS solution requires another QR decomposition:

$$X_1^* = G_1^{-\text{T}}X_1=QR=Q_1R_1~, \quad Q^{\text{T}}Q=I~, \quad Q_1^{\text{T}}Q_1=I_1~.$$
$$\hat{\beta} = (X_1^{*\text{T}}X_1^*)^{-1}X_1^{*\text{T}}y_1^* = R_1^{-1}Q_1^{\text{T}}G_1^{-\text{T}}y_1~.$$
and for prediction,

$$\hat{y}_1=X_1\hat{\beta}=G_1^{\text{T}}X_1^*\hat{\beta} = X_1^*(X_1^{*\text{T}}X_1^*)^{-1}X_1^{*\text{T}}y_1^* = G_1^{\text{T}}Q_1Q_1^{\text{T}}G_1^{-\text{T}}y_1~.$$

To summarize, there is an SVD of $K=UDU^{\text{T}}$ done _once_ for the project, and stored. There is a QR decomposition of $C^{-1}U_1^{\text{T}}=FG=F_1G_1$, with $C$ depending on $\gamma = \sigma_g^2 / \sigma^2$ and $U_1$ being the subset of rows of $U$ for the included individuals. Finally, there is a QR decomposition of $G_1^{-\text{T}}X_1=QR=Q_1R_1$. Both QR decompositions need to be done many times.

## Implementation

Thus, to solve the sub-problem, we use the SVD for all the data, divide the eigen matrix $U$ horizontally, and do a QR decomposition on $U_1^{\text{T}}$ to obtian $G_1^{-\text{T}}$ for premultiplying $y_1$ and $X_1$. Some care will need to be taken about the condition number of matrices, but that is already incorporated into `R/qtl2scan` routines. 

The `R/qtl2scan` code uses `K` for the kinship matrix and `hsq` for heritability ($\sigma_g^2/(\sigma_g^2+\sigma^2)$). The decomposed eigen object is `Ke`. Decomposition is done in 
`qtl2scan::decomp_kinship()`. Three un-exported (and one exported) routines call this, `qtl2scan:::scan1_pg()`, `qtl2scan:::scan1coef_pg()`, `qtl2scan:::scan1blup_pg()` and `qtl2scan::est_herit()`. 

They calculate heritability `hsq` with `qtl2scan:::calc_hsq_clean()` using internal `by_chr_func()`, which is unexported and found in file `scan1_pg.R`. This is called with the eigen decomposition stored in the `kinship` object, as `kinship$vectors` and `kinship$values`. For `scan1()`, calculations are done in `qtl2scan:::scan1_pg_clean()` and flavors of `scan_pg_onechr()`, which are written in Cpp.

It should be possible to have one routine that calls `qtl2scan::decomp_kinship()` and `qtl2scan:::calc_hsq_clean()` if needed, used for the four instances identified above. We would also want to either use these or something similar as exported routine to precompute SVD once for later use.
